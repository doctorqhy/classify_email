from numpy import *

def loadSimpData():
    dataMat = matrix([
        [1.,2.1],
        [2.,1.1],
        [1.3,1.],
        [1.,1.],
        [2.,1.]
    ])
    classLabels = [1.0,1.0,-1.0,-1.0,1.0]
    return dataMat,classLabels

def stumpClassify(dataMatrix,dimen,threshVal,threshIneq):

    #初始化所有类别均为1
    retArray = ones((shape(dataMatrix)[0],1))

    if threshIneq == 'lt':

        # 小于分界点的数据分为-1类
        retArray[dataMatrix[:,dimen] <= threshVal] = -1.0

    else:

        #大于分界点的数据分为-1类
        retArray[dataMatrix[:,dimen] > threshVal] = -1.0

    return retArray


#得到出错率最低的弱分类器
#D为初始权值，初始权值的大小相同
def buildStump(dataArr,classLabels,D):

    dataMatrix = mat(dataArr)
    labelMat = mat(classLabels).T
    m,n = shape(dataMatrix)
    #自定义划分区间
    numStepts = 10.0
    bestStump = {}
    bestClasEst = mat(zeros((m,1)))
    minError = inf

    for i in range(n):

        #得到当前特征的最小值和最大值以及自定义的步长
        rangeMin = dataMatrix[:,i].min()
        rangeMax = dataMatrix[:,i].max()
        stepSize = (rangeMax - rangeMin) / numStepts

        #依次取得所有分界点的值
        for j in range(-1,int(numStepts) + 1):

            #得到大于分界点划分为-1类和小于分界点分类划分为-1类的错误率
            for inequal in ['lt','gt']:

                #得到边界值
                threshVal = rangeMin + float(j) * stepSize
                #得到预测的分类
                predictedVals = stumpClassify(dataMatrix,i,threshVal,inequal)

                errArr = mat(ones((m,1)))
                #实际分类与预测分类相同时，赋值为0
                errArr[predictedVals == labelMat] = 0
                #乘以权值得到分类的加权错误率
                weightedError = D.T * errArr

                #更新最小错误率，分类特征，分界点，分解标志（大于或者小于）
                if weightedError < minError:

                    minError = weightedError
                    bestClasEst = predictedVals.copy()
                    bestStump['dim'] = i
                    bestStump['ineq'] = inequal
                    bestStump['thre'] = threshVal

    return bestStump,minError,bestClasEst


#得到弱分类器的集合
def adaBoostTrainDS(dataArr,classLabels,numIt = 40):

    weakClassArr = []
    m = shape(dataArr)[0]
    #初始化权重，使得权重相同
    D = mat(ones((m,1)) / m)
    aggClassEst = mat(zeros((m,1)))

    for i in range(numIt):

        #每迭代一次得到当前权重下出错率最低的弱分类器
        bestStump,error,classEst = buildStump(dataArr,classLabels,D)
        #print(D)
        #得到系数的值
        alpha = float(0.5 * log((1.0 - error) / max(error,1e-16)))
        bestStump['alpha'] = alpha
        #将弱分类添加到集合中
        weakClassArr.append(bestStump)
        #print(classEst.T)
        #如果为同一类则为alpha否则为-alpha
        expon = multiply(-1 * alpha * mat(classLabels).T,classEst)
        #根据公式得到新的权重集合
        D = multiply(D,exp(expon))
        D = D / D.sum()
        #组合基本分类器为强分类器
        aggClassEst += alpha * classEst
        #print(aggClassEst.T)
        #得到错误分组的个数
        aggErrors = multiply(sign(aggClassEst) != mat(classLabels).T,ones((m,1)))
        #得到出错率
        errorRate = aggErrors.sum() / m
        #print(errorRate)

        #当出错率为0时停止迭代或者迭代要求次数后停止迭代
        if errorRate == 0.0:

            break

    return weakClassArr

def adaClassify(dataToClass,classifierArr):

    dataMatrix = mat(dataToClass)
    m = shape(dataMatrix)[0]
    aggClassEst = mat(zeros((m,1)))
    for i in range(len(classifierArr)):

        classEst = stumpClassify(dataMatrix,classifierArr[i]['dim'],classifierArr[i]['thre'],classifierArr[i]['ineq'])
        #boosting的分类结果是基于所有分类器加权求和的结果
        aggClassEst += classifierArr[i]['alpha'] * classEst
        #print(aggClassEst)

    return sign(aggClassEst)

dataArr,classLabel = loadSimpData()
classifierArr = adaBoostTrainDS(dataArr,classLabel,10)

print(adaClassify([0,0],classifierArr))
